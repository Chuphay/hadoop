{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Key Word Generator\n",
    "\n",
    "In the last [tutorial](http://nbviewer.ipython.org/github/Chuphay/hadoop/blob/master/tutorials/Inverted%20Document%20Search.ipynb) we looked at how to create an Inverted Key data structure, where we were able to quickly find which documents contained certain works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the output of that Map Reduce job to help find the key words in a specific document. If you have not already run that algorithm you should do so now.\n",
    "\n",
    "The output of that program will yield a text file with lines similar to the following:\n",
    "\n",
    "```\n",
    "this hdfs://localhost:54310/user/hduser/words/file0.txt 1 hdfs://localhost:54310/user/hduser/words/file4.txt 1\t\n",
    "with hdfs://localhost:54310/user/hduser/words/file0.txt 1\t\n",
    "is hdfs://localhost:54310/user/hduser/words/file0.txt 2 hdfs://localhost:54310/user/hduser/words/file5.txt 1 \n",
    "```\n",
    "\n",
    "Which I produced using a very limited data set. \n",
    "\n",
    "To reiterate what we are looking at, at the start of each line there is a word, this word is then followed by the files in which it appeared and the number of times it appeared in that file.\n",
    "\n",
    "We will now use this data to find the key words in a document.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the key words in a document we are generally interested in two things:\n",
    "\n",
    "1.) How many times does a particular word show up in the given document. (i.e., if a word appears more often it is probably more important)\n",
    "\n",
    "2.) How many times does a particular word show up in all the documents scanned so far. \n",
    "\n",
    "The second condition is used to weight the importance of a given word. For example, if the word \"the\" appears several times in a document, is it important? Probably not, because it also appeared in many other documents.\n",
    "\n",
    "TF-IDF is the way we balance these two ideas. TF-IDF stands for Term Frequency - Inverse Document Frequency. Term frequency refers to the number of times a particular word showed up in the document. Document frequency, on the other hand, refers to how many times that word showed up in all documents divided by the number of words in all documents. \n",
    "\n",
    "Let's take a look at a small example to see how this is done:\n",
    "\n",
    "```\n",
    "doc1: \"This is a sentence about a tree.\"\n",
    "\n",
    "doc2: \"This is a sentence about a frog.\"\n",
    "\n",
    "```\n",
    "\n",
    "To calculate the document frequency of \"a\", we see that \"a\" appears 4 times and there is a total of 14 words. Therefore the document frequency of \"a\" is 4/14.\n",
    "\n",
    "The inverted document frequency, simply inverts the document frequency, so the inverted document frequency of \"a\" would be 14/4.\n",
    "\n",
    "To calculate the TF-IDF score of, for example doc2, we simply iterate over every word to find its frequency within the document and then multiply it by the inverted document frequency fot that word:\n",
    "\n",
    "```\n",
    "This: 1*(14/2) = 7\n",
    "is: 1*(14/2) = 7\n",
    "a: 2*(14/4) = 7\n",
    "sentence: 1*(14/2) = 7\n",
    "about: 1*(14/2) = 7\n",
    "trees: 1*(14/1) = 14\n",
    "```\n",
    "\n",
    "And what we see is that the TF-IDF score for \"trees\" is much higher than any other word. We also want to notice that even though \"a\" appears twice in doc2, because it appears so often in all of the texts, it's TF-IDF score is low. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this version of TF-IDF will certainly work, what one will often use instead of this is the log of the document frequency, so that insted of \n",
    "```\n",
    "trees: 1*(14/1) = 14\n",
    "```\n",
    "we would have\n",
    "```\n",
    "trees: 1*log(14/1) = 2.6390573296152584\n",
    "```\n",
    "We do this because the frequency of words follows [Zipf's Law](http://en.wikipedia.org/wiki/Zipf%27s_law)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have seen how to calculate the TF-IDF score, let's get in to the implementation of the map reduce program. AS always, I recommend you program this up yourself, but you can use my code as a guide.\n",
    "\n",
    "Here's my mapper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "\n",
    "if(len(sys.argv) != 2):\n",
    "    print \"Proper usage: python tfidf_mapper.py textfile.txt\"\n",
    "    sys.exit(1)\n",
    "\n",
    "myFile = open(sys.argv[1]).read()\n",
    "words = [i for i in re.split(r'\\W+',myFile.lower()) if i]\n",
    "myWords = {}\n",
    "for word in words:\n",
    "    try:\n",
    "        myWords[word]['num'] += 1\n",
    "    except KeyError:\n",
    "        myWords[word] = {'num': 1, 'tf': 0}\n",
    "\n",
    "#print myWords\n",
    "\n",
    "total = 0\n",
    "for line in sys.stdin:\n",
    "    internal_words = line.split()\n",
    "    internal_numbers = [int(myNum) for i, myNum in enumerate(internal_words[1:]) if i%2 == 1]\n",
    "    l = sum(internal_numbers)\n",
    "    total += l\n",
    "    try:\n",
    "        myWords[internal_words[0]]['tf'] = l\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "l = len(words)\n",
    "for i in myWords:\n",
    "    print i, myWords[i]['num'], l, myWords[i]['tf'], total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is my reducer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "myDict = {}\n",
    "\n",
    "for line in sys.stdin:\n",
    "    word, num, tot_num, tf, tf_tot = line.split()\n",
    "    num = int(num)\n",
    "    tf = int(tf)\n",
    "    tot_num = int(tot_num)\n",
    "    tf_tot = int(tf_tot)\n",
    "\n",
    "    try:\n",
    "        myDict[word]['tf'] += tf\n",
    "        myDict[word]['tf_tot'] += tf_tot\n",
    "    except KeyError:\n",
    "        myDict[word] = {'num': num, 'tot_num': tot_num, 'tf': tf, 'tf_tot': tf_tot}\n",
    "\n",
    "\n",
    "\n",
    "tfidf_score = []\n",
    "word = []\n",
    "for i in myDict:\n",
    "    word.append(i)\n",
    "    num = float(myDict[i]['num'])\n",
    "    bigN = float(myDict[i]['tf_tot'])\n",
    "    smallN = float(myDict[i]['tf'])\n",
    "    myLog = np.log(bigN/(smallN+1))\n",
    "    tfidf_score.append(num*myLog)\n",
    "\n",
    "\n",
    "out = sorted(zip(tfidf_score, word), reverse = True)\n",
    "for i in out:\n",
    "    print i[1], i[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Project Gutenberg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Project Gutenberg](https://www.gutenberg.org/) is a website with a tom of free open-source books. We were able to download a alarge selection of books from that site in order to run the key-generator algorithm with them.\n",
    "\n",
    "First we used the inverted term map reduce job from the last tutorial to set up an inverted term data structure. After that we used the TF-IDF Map-Reduce program presented in this tutorial, and used it on \"Moby Dick\". Here are the ten words that key word generator algorithm found:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
