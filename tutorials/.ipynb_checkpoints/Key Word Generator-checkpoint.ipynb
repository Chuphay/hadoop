{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Key Word Generator\n",
    "\n",
    "In the last [tutorial](http://nbviewer.ipython.org/github/Chuphay/hadoop/blob/master/tutorials/Inverted%20Document%20Search.ipynb) we looked at how to create an Inverted Key data structure, where we were able to quickly find which documents contained certain works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the output of that Map Reduce job to help find the key words in a specific document. If you have not already run that algorithm you should do so now.\n",
    "\n",
    "The output of that program will yield a text file with lines similar to the following:\n",
    "\n",
    "```\n",
    "this hdfs://localhost:54310/user/hduser/words/file0.txt 1 hdfs://localhost:54310/user/hduser/words/file4.txt 1\t\n",
    "with hdfs://localhost:54310/user/hduser/words/file0.txt 1\t\n",
    "is hdfs://localhost:54310/user/hduser/words/file0.txt 2 hdfs://localhost:54310/user/hduser/words/file5.txt 1 \n",
    "```\n",
    "\n",
    "Which I produced using a very limited data set. \n",
    "\n",
    "To reiterate what we are looking at, at the start of each line there is a word, this word is then followed by the files in which it appeared and the number of times it appeared in that file.\n",
    "\n",
    "We will now use this data to find the key words in a document.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the key words in a document we are generally interested in two things:\n",
    "\n",
    "1.) How many times does a particular word show up in the given document. (i.e., if a word appears more often it is probably more important)\n",
    "\n",
    "2.) In how many documents does a particular word show up. \n",
    "\n",
    "The second condition is used to weight the importance of a given word. For example, if the word \"the\" appears several times in a document, is it important? Probably not, because it also appeared in many other documents.\n",
    "\n",
    "TF-IDF is the way we balance these two ideas. TF-IDF stands for Term Frequency - Inverse Document Frequency. Term frequency refers to the number of times a particular word showed up in the document. Document frequency, on the other hand, refers to how many documents contain that word divided by the total number of documents. \n",
    "\n",
    "Let's take a look at a small example to see how this is done:\n",
    "\n",
    "```\n",
    "doc1: \"This is a sentence about a tree.\"\n",
    "\n",
    "doc2: \"This is a sentence about a frog.\"\n",
    "\n",
    "doc3: \"Let's make a sentence that is totally different.\"\n",
    "\n",
    "```\n",
    "\n",
    "To calculate the document frequency of \"a\", we see that \"a\" appears in all three documents and therefore the document frequency of \"a\" is 1.\n",
    "\n",
    "To calculate the document frequency of \"tree\", we see that \"tree\" appears in only one document and therefore the document frequency of \"tree\" is 1/3.\n",
    "\n",
    "The inverted document frequency, simply inverts the document frequency, so the inverted document frequency of \"a\" would still be 1. But the inverted document frequency of \"tree\" is now 3.\n",
    "\n",
    "To calculate the TF-IDF score of, for example doc2, we simply iterate over every word to find its frequency within the document and then multiply it by the inverted document frequency for that word:\n",
    "\n",
    "```\n",
    "This: 1*(3/2) = 1.5\n",
    "is: 1*1 = 1\n",
    "a: 2*1 = 2\n",
    "sentence: 1*1 = 1\n",
    "about: 1*(3/2) = 1.5\n",
    "tree: 1*3 = 2\n",
    "```\n",
    "\n",
    "And what we see is that the TF-IDF score for \"tree\" is higher than any other word. We also want to notice that even though \"a\" appears twice in doc2, because it appears so often in all of the texts, it's TF-IDF score is not as high as that of \"tree\". \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this version of TF-IDF will certainly work, what one will often use instead of this is the log of the document frequency, so that insted of \n",
    "```\n",
    "trees: 1*3 = 3\n",
    "```\n",
    "we would have\n",
    "```\n",
    "trees: 1*log(3) = 1.09\n",
    "```\n",
    "We do this because the frequency of words follows [Zipf's Law](http://en.wikipedia.org/wiki/Zipf%27s_law)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have seen how to calculate the TF-IDF score, let's get in to the implementation of the map reduce program. As always, I recommend you program this up yourself, but you can use my code as a guide.\n",
    "\n",
    "Here's my mapper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "\n",
    "if(len(sys.argv) != 2):\n",
    "    print \"Proper usage: python tfidf_mapper.py textfile.txt\"\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "myFile = open(sys.argv[1]).read()\n",
    "words = [i for i in re.split(r'\\W+',myFile.lower()) if i]\n",
    "myWords = {}\n",
    "for word in words:\n",
    "    try:\n",
    "        myWords[word]['num'] += 1\n",
    "    except KeyError:\n",
    "        myWords[word] = {'num': 1, 'tf': 0}\n",
    "\n",
    "\n",
    "documents = set()\n",
    "for line in sys.stdin:\n",
    "    internal_words = line.split()\n",
    "    for i, q in enumerate(internal_words[1:]):\n",
    "    if (i%2 == 0):\n",
    "        documents.add(q)\n",
    "    l = (len(internal_words) - 1)/2\n",
    "    try:\n",
    "        myWords[internal_words[0]]['tf'] += l\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "l = len(words)\n",
    "for i in myWords:\n",
    "    print i, myWords[i]['num'], l, myWords[i]['tf'], len(documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is my reducer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "myDict = {}\n",
    "\n",
    "for line in sys.stdin:\n",
    "    word, num, tot_num, tf, tf_tot = line.split()\n",
    "    num = int(num)\n",
    "    tf = int(tf)\n",
    "    tot_num = int(tot_num)\n",
    "    tf_tot = int(tf_tot)\n",
    "\n",
    "    try:\n",
    "        myDict[word]['tf'] += tf\n",
    "        myDict[word]['tf_tot'] += tf_tot\n",
    "    except KeyError:\n",
    "        myDict[word] = {'num': num, 'tot_num': tot_num, 'tf': tf, 'tf_tot': tf_tot}\n",
    "\n",
    "\n",
    "\n",
    "tfidf_score = []\n",
    "word = []\n",
    "for i in myDict:\n",
    "    word.append(i)\n",
    "    num = float(myDict[i]['num'])\n",
    "    bigN = float(myDict[i]['tf_tot'])\n",
    "    smallN = float(myDict[i]['tf'])\n",
    "    myLog = np.log(bigN/(smallN+1)) #Add +1 to avoid divide by zero\n",
    "    tfidf_score.append(myLog*num)\n",
    "\n",
    "\n",
    "out = sorted(zip(tfidf_score, word), reverse = True)\n",
    "for i in out:\n",
    "    print i[1], i[0]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Project Gutenberg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Project Gutenberg](https://www.gutenberg.org/) is a website with a ton of free open-source books. We were able to download a a large selection of books from that site in order to run the key-generator algorithm with them.\n",
    "\n",
    "First we used the inverted term map reduce job from the last tutorial to set up an inverted term data structure. After that we used the TF-IDF Map-Reduce program presented in this tutorial, and used it on \"Moby Dick\". Here are the twenty-five top words that key word generator algorithm found with their tf-idf scores:\n",
    "\n",
    "```\n",
    "whale 1129.7864724\t\n",
    "ahab 615.230103011\t\n",
    "stubb 413.625543496\t\n",
    "queequeg 405.578353933\t\n",
    "whales 322.664711559\t\n",
    "starbuck 318.668706662\t\n",
    "sperm 293.769364256\t\n",
    "pequod 278.432758851\t\n",
    "flask 173.819294543\t\n",
    "ye 168.707248483\t\n",
    "sea 162.287099492\t\n",
    "whaling 157.720437367\t\n",
    "nantucket 154.506039594\t\n",
    "moby 143.239974207\t\n",
    "aye 142.02506344\t\n",
    "thou 138.433744041\t\n",
    "boats 134.694737586\t\n",
    "bildad 122.317281345\t\n",
    "peleg 119.09840552\t\n",
    "whalemen 115.879529695\t\n",
    "ship 115.588359581\t\n",
    "leviathan 115.581389215\t\n",
    "jonah 102.337688368\t\n",
    "harpooneer 97.5217971504\t\n",
    "spout 96.566274746\n",
    "```\n",
    "\n",
    "Which seems like a reasonable list."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
