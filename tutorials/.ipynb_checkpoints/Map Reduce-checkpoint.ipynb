{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map Reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 2003, Ghemawat and Dean wrote a program to greatly simplify the use of parallel and distributed systems, which they have outlined in their [paper.](http://static.googleusercontent.com/media/research.google.com/en/us/archive/mapreduce-osdi04.pdf) Their program was able to take care of the details of partioning the data over numerous computers, scheduling which programs ran at which time and on which machine, managing the necessary between computer communication, and managing what happens when an error occurs. Their program allows everyday programmers to use vast clusters of commodity machines without any particular experience using parallel or distributed systems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They were able to accomplish this by restricting what the program user was allowed to do. In fact, the program user is only allowed to supply two functions: a mapper and a reducer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even with this restriction, many real world programs are programmable using just map and reduce. To get a feeling for how this works, let us take a look at map and reduce separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To follow along with this tutorial, one should use Python 2.7. If you are unfortunate and somehow you are running Python 3.4 on your home computer, please SSH into almost any linux server and python 2.7 should hopefully be available. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map is a pretty straightforward concept, but it was slightly miss-named. A much better name would have been apply, because what we will be doing is applying a function to a list. Let's see how this works.\n",
    "\n",
    "Suppose you are given a list of numbers like \n",
    "\n",
    "```\n",
    "1 2 3 4 5 6 7 8 9 10 11\n",
    "```\n",
    "and you want to square each of the numbers.\n",
    "\n",
    "What you need to do then is to apply a funtion that squares each number, so that the final output is\n",
    "\n",
    "```\n",
    "1 4 9 16 25 36 49 64 81 100 121\n",
    "```\n",
    "And, believe it or not, that is essentially all that a map function does. Let's program that up in Python: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 9, 16, 25, 36, 49, 64, 81, 100, 121]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#First, we start off with the list of numbers:\n",
    "x = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
    "\n",
    "#Now, we simply define the function that we want to apply to our list:\n",
    "def square(x):\n",
    "    return x*x\n",
    "\n",
    "#Finally, we apply our function to each member of the list:\n",
    "map(square, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can tell, map is really straight forward. We create a function, and then we apply that function to every element in the data set that we are processing. Let us move on to the Reduce function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce is only slightly more complicated than map. The way I think of it is as a cummulative adder.\n",
    "\n",
    "For example, if we want to add all the numbers in a list:\n",
    "```\n",
    "1 2 3 4 5 6\n",
    "```\n",
    "Then the reducer scans through the numbers and calculates the output in the following manner:\n",
    "```\n",
    "1 + 2 = 3\n",
    "3 + 3 = 6\n",
    "6 + 4 = 10\n",
    "10 + 5 = 15\n",
    "15 + 6 = 21\n",
    "```\n",
    "Let's try it in Python to see how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#First, we set up the input:\n",
    "x = [1, 2, 3, 4, 5, 6]\n",
    "\n",
    "#Next, we must write the function that we will reduce with\n",
    "def add(x, y):\n",
    "    return x + y\n",
    "\n",
    "#Finally, we simply call reduce:\n",
    "reduce(add, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducing is not much harder than mapping, but complicated reducers can take time to wrap your head around. Basically a reducer will take a list of numbers and gather them together one-by-one  by using the function supplied to the reducer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at an easy example to get some practice with what we are doing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Map Reduce to Multiply Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you are given two vectors, x and y, and we need to calculate their dot product, i.e., suppose\n",
    "\n",
    "$$x = \\left(\\begin{matrix}2\\\\ 6\\\\ 3 \\end{matrix}\\right) ~~~~~ y = \\left(\\begin{matrix}-4\\\\ 2\\\\ 1\n",
    "\\end{matrix}\\right)$$\n",
    "\n",
    "Then the dot product is simply\n",
    "\n",
    "$$ x^T \\cdot y  = \\left(\\begin{matrix}2& 6& 3 \\end{matrix}\\right)\\cdot \\left(\\begin{matrix}-4\\\\ 2\\\\ 1\n",
    "\\end{matrix}\\right) = -8 + 12 +3= 7$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To break this problem down into a map reduce job, we will make the mapper multiply the individual elements together and make the reducer add up the results: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#First I put the input into a form that is easy for the mapper to digest\n",
    "x = [(2,-4), (6,2), (3,1)]\n",
    "\n",
    "#Next, I create my mapper,\n",
    "#keeping in mind that my input will be a tuple of the form (2, -4)\n",
    "def multiply(x):\n",
    "    return x[0]*x[1]\n",
    "\n",
    "#Then I simply run the mapper:\n",
    "out = map(multiply, x)\n",
    "\n",
    "#Now, we will simply add up the numbers from the mapper\n",
    "#I will use the adder that I already made previously to perform this job:\n",
    "reduce(add, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully that last example seemed straightforward. \n",
    "\n",
    "The next example is a little more complex, but is probably the most famous map reduce example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure that we fully understand Map-Reduce, let's take a look at the famous word count problem.\n",
    "\n",
    "Our task is simple, we are given a document, and we want to find out how many times each word appeared in the document. Suppose we are given the following document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = \"This is a sentence about cats. This is a sentence about dogs. This is not about anything.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task, our mapper is essentially trivial. It will simply go through every word in the document and do nothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', 'a', 'sentence', 'about', 'cats.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#First, we turn the string into a list for our mapper to work upon:\n",
    "x = s.split()\n",
    "\n",
    "#Next, we make the function we will apply:\n",
    "def doNothing(x):\n",
    "    return x\n",
    "\n",
    "#Finally, we apply map to the list and save it:\n",
    "output = map(doNothing, x)\n",
    "\n",
    "#Let's print the first few words in our output to make sure it is doing what it is supposed to do:\n",
    "output[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Between the map step and the reduce step, it is often advantagous to sort the output from the mapper. In fact, when using Hadoop, this is the standard behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output = sorted(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the reduce step, takes this sorted output and adds up the occurences of each word and returns a list of the word and the count.\n",
    "\n",
    "We will take advantage of the fact that the input is sorted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This 3\n",
      "a 2\n",
      "about 3\n",
      "anything. 1\n",
      "cats. 1\n",
      "dogs. 1\n",
      "is 3\n",
      "not 1\n",
      "sentence 2\n"
     ]
    }
   ],
   "source": [
    "#First we setup some global variables used to keep track of which word we are currently processing\n",
    "#and how many times we have already processed this word\n",
    "current_word= None\n",
    "count = 0\n",
    "\n",
    "#We will go through each element outputed by the mapper\n",
    "for word in output:\n",
    "    \n",
    "    #If this is the first word of the list, then current_word is still initialized as None\n",
    "    #We need to set it to be the first word in the list\n",
    "    if current_word == None:\n",
    "        current_word = word\n",
    "        \n",
    "    #Is this the word we are currently processing?\n",
    "    #If so, then we need to increment the count\n",
    "    if current_word == word:\n",
    "        count += 1\n",
    "        \n",
    "    #If not, then we have finished processing the current_word\n",
    "    #So we need to print the current_word and the count\n",
    "    #and re-initialize the current_word and the count\n",
    "    else:\n",
    "        print current_word, count\n",
    "        current_word = word\n",
    "        count = 1\n",
    "        \n",
    "#Finally, the last word and its count must be printed        \n",
    "print current_word, count        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I mentioned earlier, sometimes it takes some effort to wrap your head around what the reducer is doing. However, I would highly recommend trying to figure out how this reducer works as it will be the basis for many of the other reducers you will see in the other tutorials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's take this map-reduce algorithm and use it in conjunction with Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hadoop and Map-Reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to try using Map and Reduce in conjunction with Hadoop. \n",
    "\n",
    "I am assuming that you are on a server running Hadoop. To check, try typing \"jps\" into the terminal, you should see something like the following\n",
    "```\n",
    "$ jps\n",
    "6068 NodeManager\n",
    "5568 DataNode\n",
    "5940 ResourceManager\n",
    "5444 NameNode\n",
    "6123 Jps\n",
    "5785 SecondaryNameNode\n",
    "```\n",
    "\n",
    "If you do not, get your system administrator to figure out what is wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need some files to work with. Try the following:\n",
    "\n",
    "```\n",
    "$ echo \"This is one sentence.\" > text1.txt\n",
    "$ echo \"This is another sentence.\" > text2.txt\n",
    "```\n",
    "\n",
    "and continue doing that until you have several text files.\n",
    "\n",
    "Now we need to move these text files into the Hadoop Distributed FIle System (HDFS). First, we should make a folder:\n",
    "\n",
    "```\n",
    "$ hadoop dfs -mkdir text\n",
    "```\n",
    "Then we need to move the files into that folder\n",
    "\n",
    "```\n",
    "$ hadoop dfs -put text*.txt text/\n",
    "```\n",
    "\n",
    "Now you should be able to check that they are all there by typing\n",
    "\n",
    "```\n",
    "$ hadoop dfs -ls text/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to make our mapper, type\n",
    "\n",
    "```\n",
    "$ emacs word_count_mapper.py\n",
    "```\n",
    "\n",
    "and input the following lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "for line in sys.stdin:\n",
    "    words = line.split()\n",
    "    for word in words:\n",
    "        print word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you are wondering, the way Hadoop works with Python is that Hadoop will send the contents of the current file to the \"standard input\" (If you don't know what that means, don't worry about it) and python will then collect that information via \"sys.stdin\".\n",
    "\n",
    "After you have finished typing that in, type ctrl+x and then ctrl+s to save and then ctrl+x and then ctrl+c to exit emacs.\n",
    "\n",
    "Now we need to make our reducer, type \n",
    "\n",
    "```\n",
    "$ emacs word_count_reducer.py\n",
    "```\n",
    "and input the following lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "current_word= None\n",
    "count = 0\n",
    "\n",
    "for word in sys.stdin:\n",
    "    word = word.strip() #Remove trailing whitespace\n",
    "    \n",
    "    if current_word == None:\n",
    "        current_word = word\n",
    "\n",
    "    if current_word == word:\n",
    "        count += 1\n",
    "\n",
    "    else:\n",
    "        print current_word, count\n",
    "        current_word = word\n",
    "        count = 1\n",
    "     \n",
    "print current_word, count    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that is all saved up, we are ready to run the map reduce job. Type the following lines into the terminal:\n",
    "\n",
    "\n",
    "```\n",
    "$ hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.6.0.jar -file word_count_mapper.py -mapper \"python word_count_mapper.py\" -file word_count_reducer.py -reducer \"python word_count_reducer.py\" -input text/* -output text_out\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything worked correctly, the last line of output should read\n",
    "\n",
    "```\n",
    "15/05/18 16:16:26 INFO streaming.StreamJob: Output directory: text_out\n",
    "```\n",
    "If not, you will have to comb through the output and try to figure out what went wrong.\n",
    "\n",
    "Note, a common mistake is that there may already be a folder called text_out in HDFS, if that's the case then simply change the output folder to text_out2 or something equivalent.\n",
    "\n",
    "Finally, if it all ran correctly, we can check the output with the following command:\n",
    "\n",
    "```\n",
    "$ hadoop dfs -cat text_out/*\n",
    "```\n",
    "\n",
    "And hopefully, it outputed what you expected."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
